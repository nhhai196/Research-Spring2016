%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for 
% including it in another document. To do this, you have two options: 
%
% 1) Copy/paste everything between \begin{document} and \end{document} 
% starting at \begin{titlepage} and paste this into another LaTeX file where you 
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and 
% move this file to the same directory as the LaTeX file you wish to add it to. 
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{book}
\usepackage{amsmath,amsfonts,amssymb,amsthm,titling,url,array}
\usepackage{mathtools}

\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\usepackage[margin=1.0in]{geometry}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter] % reset theorem numbering for each chapter

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Purdue University}\\[1.5cm] % Name of your university/college
\textsc{\Large CS 699}\\[0.5cm] % Major heading such as course name
\textsc{\large Spring 2016}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Research Thesis}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Hai \textsc{Nguyen} % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Instructor:} \\
Prof. Hemanta \textsc{Maji} % Supervisor's Name
\end{flushright}
\end{minipage}\\[4cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[3cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

%\includegraphics{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

%----------------------------------------------------------------------------------------
\chapter{Fourier Basics}
\section{Vector Space of Functions on Boolean Hyper-cube}
\begin{definition}[Inner Product]
Consider the $2^n$-dimensional vector space of all functions 
$f : \{ 0,1 \}^n \rightarrow\mathbb{R}$. 
We define an inner product on this space by 
$$\langle f, g \rangle 
\defeq \mathbb{E}[f \cdot g] 
= \frac{1}{2^n}\sum\limits_{x  \in \{ 0,1 \}^n} f(x)g(x)$$. 
\end{definition}
\section{Characteristic Functions}
\begin{definition} [Characteristic function]
For each $S \subseteq [n] = \{1,2,..., n\}$, we define the characteristic function  of $S$ as 
$${\chi}_S(x) = (-1)^{S \cdot x}, \text{where } S \cdot x = \sum\limits_{i=1}^{n} S_i \cdot x_i = \sum \limits_{i \in S} x_i $$.
\end{definition}

\begin{lemma}
For every $S \subseteq [n],$ \center
$\sum\limits_{x \in \{ 0, 1 \}^n} \chi_S(x) =
\begin{cases}
2^n      & \text{if} \ S = \emptyset \\
0        & \text{if} \ S \neq \emptyset
\end{cases}$
\end{lemma}

\begin{proof}
If $ S = \emptyset $, then $S \cdot x = 0$. So $\sum\limits_{x \in \{ 0, 1 \}^n} \chi_S(x) = \sum \limits_{x \in \{ 0, 1 \}^n} 1 = 2^n $.\\
If $S \neq \emptyset$, then there exists $k$ such that $S_k \neq 0$. Hence, \\
\begin{align*}
\sum\limits_{x \in \{ 0, 1 \}^n} \chi_S(x) 
& = \sum\limits_{x \in \{ 0, 1 \}^n} (-1)^{\sum \limits_{i \in S} x_i} \\
& = \sum\limits_{x \in \{ 0, 1 \}^n} [(-1)^{x_k} \cdot (-1)^{\sum\limits_{i \in S \setminus \{k \}} x_i}] \\
& = \sum\limits_{x_k \in \{0, 1 \} }(-1)^{x_k} 
\cdot \sum\limits_{x \setminus x_k \in \{0,1\}^{n-1}} (-1)^{\sum\limits_{i \in S \setminus \{k \}} x_i}  \\
& = \left[(-1)^0 + (-1)^1 \right] \sum\limits_{x \setminus x_k \in \{0,1\}^{n-1}} (-1)^{\sum\limits_{i \in S \setminus \{k \}} x_i} \\
& = 0
\end{align*}
\end{proof}

\begin{theorem}
For every $S, T \subseteq [n]$,
\center
$\langle \chi_S, \chi_T \rangle =
\begin{cases}
1      & \text{if} \ S = T \\
0      & \text{if} \ S \neq T
\end{cases}$
\end{theorem}

\begin{proof}
\begin{align*}
\langle \chi_S, \chi_T \rangle 
= \frac{1}{2^n}  \sum\limits_{x \in \{0,1\}^n} (-1)^{S \cdot x + T \cdot x}
= \frac{1}{2^n}  \sum\limits_{x \in \{0,1\}^n} (-1)^{(S \Delta T) \cdot x}
\end{align*}
where $\Delta$ is the symmetric different between two sets $S$ and $T$. \\
 $S \Delta T = \emptyset$ if and only if  $S = T$. Hence, our goal follows immediately from Lemma 1.3.
\end{proof}

\section{Fourier Basis}
\begin{theorem}
The set of all $\chi_S$ defines an orthonormal basis for the space of all real-valued function on $\{ 0, 1\}^n$
\end{theorem}

\begin{proof}
From Theorem 1.4, the set of all $\chi_S$ is an orthonormal set. Also, there are $2^n$ different $\chi_S$. Hence, the set of all $\chi_s$ must be an orthonormal basis for the space of all real-valued functions on $\{ 0,1 \}^n$.
%it suffices to show that the set of all $\chi_S$ is a basis. Suppose 
%$\sum\limits_{S \subseteq [n]}a_S \chi_S = \textbf{0}$. We will show that $X_S$ are %linearly independent by proving $a_S = 0$ for every $S$. Let $S$ be a subset of $[n]$, then 
%\begin{align*}
%          &= \chi_S \sum\limits_{T \subseteq [n]} a_T \chi_T 
%	       = \sum\limits_{T \subseteq [n]} a_T \langle \chi_S, \chi_T \rangle \\
%	       &= a_S \langle \chi_S, \chi_S \rangle +
%	          \sum\limits_{T \subseteq [n], T \neq S} a_T \langle \chi_S, \chi_T \rangle 
%	       = a_S  	       	  
%\end{align*}
%Also, there are $2^n$ different $X_S$. Hence, the set of all $\chi_S$ is a basis for the space of all real-valued function on $\{ 0,1 \}^n$
\end{proof}
The set of all $\chi_S$ is called the \textit{the Fourier basis}.

\section{Fourier Transform}
\begin{definition} 
For each $S \subseteq [n]$, we define the Fourier transform of $f$ at $S$ as following: 
 $$\widehat{f}(S) \defeq \mathbb{E}[f \cdot \chi_S] = \langle f, \chi_S \rangle $$
\end{definition}

\begin{definition}
[Fourier transform] The mapping $\mathcal{F} : f \mapsto \widehat{f}$ is called the Fourier transform. 
\end{definition}

If we view functions $f, \widehat{f}$ as $N$-dimensional vectors, then we can write the Fourier transform as the product of $f$ and some matrix $F$ as following: \\
\begin{align*}
\mathcal{F}(f) 
& = f \cdot F \\
& = \frac{1}{N} \left( f(0), f(1), \cdots , f(N-1) \right)
	 \left( \begin{array}{cccc}
		\chi_0(0)  &  \chi_1(0)   & \cdots  & \chi_{N-1}(0) \\
		\chi_0(1)  &  \chi_1(1)   & \cdots  & \chi_{N-1}(1) \\
		\vdots     & \vdots       & \ddots  & \vdots       \\
	    \chi_0(N-1)& \chi_1(N-1)  & \cdots  & \chi_{N-1}(N-1)	
\end{array} \right) \\ 
&= (\widehat{f}(0), \widehat{f}(1), \cdots, \widehat{f}(N-1)) \\
& = \widehat{f}
\end{align*}
where $F_{ij} = \frac{\chi_i(j)}{N}$. Since $\chi_i(j) = \chi_j(i)$, $F$ is symmetric. 

\begin{lemma}
$F$ is invertible
\end{lemma}

\begin{proof}
We have 
$$(F \cdot F)_{ij} = \frac{1}{N^2}\sum\limits_{k=0}^{N-1} 
 \chi_i(k) \cdot \chi_j(k) = \frac{1}{N} \langle \chi_i, \chi_j \rangle $$ 
Based on Theorem 1.4, it is easy to see that $$(F \cdot F)_{ij} = 
\begin{cases}
	 \frac{1}{N} & \text{ if } i = j \\
	 0             & \text{otherwise} \end{cases} $$
So $F \cdot F = N \cdot I$, which implies that $F$ is invertible
\end{proof}

\begin{theorem}
The mapping $\mathcal{F}$ is linear.
\end{theorem}

\begin{proof}
This follows from the properties of inner product. For any $S$,
$$\widehat{af+bg}(S) =
 \langle af + bg, \chi_S \rangle = 
 \langle af, \chi_S \rangle + \langle bg, \chi_S \rangle = 
 a\langle f, \chi_S \rangle + b \langle g, \chi_s \rangle =
 a \widehat{f}(S) + b \widehat{g}(S) $$
Thus, $\widehat{af+bg} =   a \widehat{f} + b \widehat{g}$, which means $
\mathcal{F}$ is linear. \\
Here is another way to show that.
 $$\widehat{af+bg} = \mathcal{F}(af + bg) = (af+ bg) F = 
 af F + bg  F = a(f F) + b (g F) = a\widehat{f} + b \widehat{g} $$
\end{proof}

\begin{theorem}
The linear map $\mathcal{F}$ is a bijection.
\end{theorem} 

\begin{proof}
It suffices to show that $F$ is invertible, which follows immediately from Lemma 1.8.
\end {proof}
\section{Parseval's Identity}
Since the set of $\chi_S$ forms an orthonormal basis, 
\begin{equation}
f = \sum \limits_{S} \widehat{f}(S) \chi_S.
\end{equation}
Hence,
\begin{equation}
\langle f, g \rangle = \sum\limits_{S} \widehat{f}(S) \widehat{g}(S)
\end{equation}
In particular, when f = g we get Parseval's identity:
\begin{equation}
\| f \|_2^2 = \sum\limits_{S} \widehat{f}(S)^2
\end{equation}
This also implies:
\begin{equation}
\| f - g \|_2^2 = \sum\limits_{S} (\widehat{f}(S) - \widehat{g}(S))^2
\end{equation}

%Suppose $\mathcal{F}(f_1) = \mathcal{F}(f_2)$, i.e, $\widehat{f_1}(S) = \widehat{f_2}(S)$ for every $S$, then it is followed from equation (1.1) that $f_1 = f_2$. So $\mathcal{F}$ is injective. \\
%Also, equation (1.1) implies that for every $\widehat{f}$, there exists a function $f = \sum \limits_{S} \widehat{f}(S) \chi_S $ such that $\mathcal{F}(f) = \widehat{f}$, which means that $\mathcal{F}$ is surjective. \\
%Thus, $\mathcal{F}$ is a bijection as derived 

\section{Convolution}
\begin{definition}
Given any two function $f$ and $g$ : $\{ 0,1 \}^n \rightarrow \mathbb{R}$, the convolution of $f*g$ :  $\{ 0,1 \}^n \rightarrow \mathbb{R}$ is defined as
$$(f*g)(x) \defeq \frac{1}{2^n} \sum\limits_{y \in \{0,1\}^n} f(x \oplus y) g(y)$$
\end{definition}

\begin{theorem}
If $X$ and $Y$ are $n$-bits random independent variables with probability distributions $f$ and $g$, respectively, then $2^n(f*g)$ is the distribution of the random variable $Z = X \oplus Y$.
\end{theorem}

\begin{proof}

\begin{align*}
Pr[Z = z] 
& = Pr[X = z \oplus Y] \\
& = \sum\limits_{y \in \{0,1\}^n} Pr[X = z \oplus y | Y = y] 	 \\
& = \sum\limits_{y \in \{0,1\}^n} Pr[X = z \oplus y] \cdot Pr[Y =y] \\
& = \sum\limits_{y \in \{0,1\}^n} f((z \oplus y) \cdot g(y) \\
& = 2^n (f *g)(z)
\end{align*}
\end{proof}
\begin{theorem}
For every $S \subseteq [n]$, 
$$\widehat{f*g}(S) = \widehat{f}(S) \cdot \widehat{g}(S)$$
\end{theorem}

\begin{proof}
\begin{align*}
\widehat{f*g}(S) 
& = \frac{1}{2^n}\sum \limits_{x} (f*g)(x) \chi_S(x) \\
& = \frac{1}{2^n} \sum \limits_{x} \left( \frac{1}{2^n} 
	\sum\limits_{y} f(x \oplus y) g(y) \right) \chi_S(x) \\
& = \frac{1}{2^{2n}} \sum\limits_x  \sum\limits_y f(x \oplus y) g(y) 
	\chi_S(x \oplus y) \chi_S(y) \\
& = \frac{1}{2^n}\sum \limits_{x} f(x \oplus y) \chi_S(x \oplus y) 
	\left( \frac{1}{2^n}\sum \limits_{y} g(y) \chi_S(y) \right)\\
& = \widehat{f}(S) \cdot \widehat{g}(S)		
\end{align*}
\end{proof}

Intuitively, the convolution $f * g$ is the product of the Fourier transforms of $f$ and $g$.
\begin{theorem}
Let $V$ be a subspace of dimension $k$ of $\{ 0, 1 \}^n$ and let $V^\perp$ be the dual of $V$. Define \\
 $$f(x) =
\begin{cases}
	\frac{1}{2^k} & \text{if} \ x \in V \\
	0             & \text{otherwise}.
\end{cases}$$. \\
Then
$$\widehat{f}(S) =
\begin{cases}
	\frac{1}{N} & \text{if} \ S \in V^\perp \\
	0             & \text{otherwise}.
\end{cases}$$.
\end{theorem}

\begin{proof}
Suppose $v_1, v_2,..., v_k $ is a basis of $V$.

\begin{claim}
$V = \langle v_1 \rangle \oplus \langle v_2 \rangle 
	 \oplus ... \oplus \langle v_k \rangle $
\end{claim}

\begin{claim}
${\langle v_1 \rangle}^\perp \cap ... \cap {\langle v_k \rangle}^\perp
= {\langle v_1, ..., v_k \rangle}^\perp = V^\perp$
\end{claim}

\noindent Let $f_i = \begin{cases}
	\frac{1}{2} & \text{if} \ S \in \langle v_i \rangle \\
	0             & \text{otherwise}
\end{cases}$, 
then $\widehat{f_i} = \begin{cases}
	\frac{1}{N} & \text{if} \ S \in {\langle v_i \rangle}^ \perp \\
	0             & \text{otherwise}.
\end{cases}$. 

\noindent From above claims, we immediately obtain following result.
\begin{claim}
$f = f_1 \oplus f_2 \oplus ... \oplus f_k$
\end{claim}

\noindent Hence,
$$\widehat{f}(S) = N^{k-1} \widehat{f_1}(S) ... \widehat{f_k}(S)$$ \\
If $S \in V^\perp$, then $S \in {\langle v_i \rangle}^\perp$ for every $i$, so $\widehat{f}(S) = N^{k-1} \cdot (\frac{1}{N})^k = \frac{1}{N}$. \\
If $S \not\in V^\perp$, then there exits some $i$ such that $S \not\in {\langle v_i \rangle}^\perp$, which implies $\widehat{f_i}(S) = 0$. Hence, $\widehat{f}(S) = 0$
\end{proof}

\chapter{Min Entropy}
Let $X = (x_0, x_1, ..., x_{N-1})$ be a distribution function of a random variable over $\{ 0, 1 \}^n$, where $N \defeq 2^n$. 
\begin{definition}[Min Entropy]
We define the min entropy of $X$ as follow.\center
$H_\infty (X) \defeq \min\limits_{i} (- \log x_i)$
\end{definition}

This implies that if $H_\infty(X) \geq k \ \text{then } \ x_i \leq \frac{1}{2^k} $ for every $0 \leq i \leq N-1$

\begin{theorem}[Collision Probability]
If we sample $X$ twice, then the probability we get the same result, denoted $Col(X)$, is 
$\sum\limits_{i = 0}^{N-1} {x_i}^2 = N \cdot \| X \|_2^2$.
\end{theorem}

\begin{definition}[Flat Distribution]
A probability distribution function $f \colon \{ 0, 1 \}^n \rightarrow (0,1)$ is a \textit{T-flat} if there $\exists \ S \subseteq \{ 0,1 \}^n$ such that $|S| = T$ and 
$f(x) = 
\begin{cases}
	\frac{1}{T} & \text{if} \ x \in S \\
	0             & \text{otherwise}
\end{cases}$
\end{definition}

\begin{lemma} For every $\alpha \geq \beta$, every $\alpha$-flat distribution can be written as the sum of $\beta$-flat distributions. 
\end{lemma}

\begin{theorem}
For every integer $k \geq 0$, if $H_\infty(X) \geq k$, then 
$X = \sum \alpha_i X_i$, where each $X_i$ is a \textit{$2^k$-flat}, $\alpha_i \in [0,1] \text{ for every} \ i$, and $\sum\limits_i \alpha_i = 1$.
\end{theorem}

\begin{proof}
Let $S$ be the set of all the probability distributions $X$ with $H_{\infty}(X) \geq k$, then S is a compact (closed and bounded) convex set in $\mathbb{R}^N$.
\begin{claim} The set of all $2^k$-flat distributions is the set of all extreme points of $S$.
\end{claim}
First, every $2^k$-flat distribution $X$ is an extreme point of $S$ since if $X = \alpha Y + (1-\alpha)Z$ for some $Y, Z \in S$ and $\alpha\in [0,1]$, then $X = Y = Z$. Next, we want to show that for any $X$, which is not a $2^k$-flat distribution, $X$ is not an extreme point of $S$. Since $X$ is not a $2^k$-flat distribution, there exist $i<j$ such that $0 < x_i, x_j < \frac{1}{2^k}$. So we can choose $\delta > 0$ such that $x_i + \delta, x_j + \delta \leq \frac{1}{2^k}$ and $x_i - \delta, x_j - \delta \geq 0$. Now let $y_k = z_k = x_k$ for every $k \neq i, k \neq j$,  $y_i = x_i + \delta$, $y_j = x_j - \delta$, $z_i = x_i - \delta$, and $z_j = x_j + \delta$. Then $Y, Z \in S$ and $X = \frac{1}{2} Y + \frac{1}{2} Z$, which implies that $X$ is not an extreme point. \\ 
Back to the problem, since $S$ is a compact convex set, the set of all convex combinations of its vertices is identical to $S$. Hence, every distribution $X$ with  $H_{\infty}(X) \geq k$ can be written as a convex combination of $k$-flat distributions. 

\end{proof}

\begin{theorem}
If $H_\infty(X) \geq k$, then $Col(X) \leq \frac{1}{2^k}$.
\end{theorem}

\begin{proof}
By theorem 2.5, we can write $X$ as $X = \sum\limits_i \alpha_i X_i$, where each $X_i$ is a $2^k\text{-flat}$, $\sum \alpha_i = 1$, and $\alpha_i \in [0,1] \text{for every} \ i$. It is obvious that $Col(X_i) = \| X_i \|_2^2 = \frac{1}{2^k}$.\\
Collision functions are convex, so by Jensen's inequality, 
$$Col(X) = Col \left( \sum\limits_i \alpha_i X_i \right)
\leq \sum\limits_i \alpha_i \cdot Col(X_i)
= \sum\limits_i \alpha_i \frac{1}{2^k} 
=\frac{1}{2^k} \sum\limits_i \alpha_i 
= \frac{1}{2^k}$$
%By Cauchy-Sachwarz inequality, \\
%$\| X \|_2^2 
%= \| \sum \alpha_i X_i \|_2^2 
%\leq | \sum \alpha_i | \cdot | \sum \alpha_i {X_i}^2|
%= \sum \alpha_i \| {X_i} \|_2^2 
%= \sum \alpha_i \frac{1}{2^k}
%= \frac{1}{2^k}$ 
\end{proof}

\begin{theorem}
If $H_\infty(X) \geq k$, then $\sum\limits_{S} \widehat{X}(S)^2 \leq \frac{1}{N \cdot 2^k}$.
\end{theorem}

This follow immediately from the Parseval's identity.

\begin{definition}[Small Bias Distribution]
Let $\mathcal{D}$ be a probability distribution function over $\{ 0,1 \}^n$.
We say that $\mathcal{D}$ is $\alpha\text{-bias}$ if $\widehat{D}(S) \leq \frac{\alpha}{N}$. \\
%Fools all linear test means that if for any test $s$, a sample of $\mathcal{D}$, it returns 0 with probability $\frac{1}{2}+ \alpha$ and returns 1 with probability $\frac{1}{2} - \alpha$, where $\alpha \leq \alpha^*$
\end{definition}

%\begin{theorem}
%If $\mathcal{D}$ is a $\alpha^*\text{-bias}$, then $\widehat{\mathcal{D}}(S) \leq \frac{2\alpha^*}{N}$ for all $S$.
%\end{theorem}

%\begin{proof}
%$\widehat{\mathcal{D}}(S) = \frac{1}{N} \sum\limits_{x} \mathcal{D}(x) (-1)^{Sx} = \frac{1}{N} ((\frac{1}{2}+ \alpha) - (\frac{1}{2} - \alpha)) = \frac{2\alpha}{N} \leq \frac{2\alpha^*}{N}$ 
%\end{proof}

\begin{definition}
\textit{Statistical Different} between two distributions $A$ and $B$ is defined as follow:
$$SD(A,B) = \frac{1}{2} \sum\limits_{i} |a_i - b_i |$$
\end{definition}
\begin{theorem}
Let $\mathcal{D}$ be a small bias distribution with $\widehat{\mathcal{D}}(S) \leq \frac{\alpha}{N}$ for all $S$, let $\mathcal{M}$ be a min entropy source such that $H_\infty(\mathcal{M}) \geq k$, and let $\mathcal{U}$ be  the uniform distribution over $n$-bits string. Then $$SD(\mathcal{D} \oplus \mathcal{M}, \mathcal{U}) \leq \frac{\alpha \sqrt{N}}{2^{1+k/2}}$$
\end{theorem}

\begin{proof} 
\begin{align*}
SD(\mathcal{D} \oplus \mathcal{M}, \mathcal{U})
&= \frac{1}{2} \sum\limits_i |(\mathcal{D} \oplus \mathcal{M})(i) - \mathcal{U}(i) | \\
& \leq \frac{1}{2}  \sqrt{N \sum\limits_i [(\mathcal{D} \oplus \mathcal{M})(i) - \mathcal{U}(i)]^2} \\
&= \frac{1}{2}  \sqrt{N^2 \cdot \| (\mathcal{D} \oplus \mathcal{M}) - \mathcal{U} \|_2^2} \\
& = \frac{N}{2} \sqrt{\sum\limits_{S} 
	[\widehat{\mathcal{D} \oplus \mathcal{M}}(S) - \mathcal{U}(S)]^2} \\
& = \frac{N}{2} \sqrt{\sum\limits_{S \neq \emptyset} 
	\widehat{\mathcal{D} \oplus \mathcal{M}}(S)^2} \\
\end{align*}
%&= \frac{N}{2} \sqrt{\sum\limits_{S \neq \emptyset}
%	N^2 \cdot \widehat{\mathcal{D}}(S)^2 \cdot \widehat{\mathcal{M}}(S)^2} \\
%&\leq \frac{N}{2} \sqrt{\sum\limits_{S \neq \emptyset}
%	N^2 \cdot (\frac{\alpha}{N})^2 \cdot \widehat{\mathcal{M}}(S)^2} \\
%&= \frac{\alpha N}{2} \sqrt{\sum\limits_{S \neq \emptyset} \widehat{\mathcal{M}}(S)^2} \\
%\leq 
By convolution,
\begin{align*}
\sum\limits_{S \neq \emptyset} \widehat{\mathcal{D} \oplus \mathcal{M}}(S)^2 
&= \sum\limits_{S \neq \emptyset} N^2 \cdot \widehat{\mathcal{D} * \mathcal{M}} (S)^2 \\
&= N^2 \sum\limits_{S \neq \emptyset} \widehat{\mathcal{D}}(S)^2 \cdot \widehat{\mathcal{M}}(S)^2 \\
& \leq N^2 \cdot  \sum\limits_{S \neq \emptyset} (\frac{\alpha}{N})^2 \cdot \widehat{\mathcal{M}}(S)^2 \\
&= \alpha^2 \cdot \sum\limits_{S \neq \emptyset} \widehat{\mathcal{M}}(S)^2 \\
& \leq \frac{\alpha^2}{N \cdot 2^k}
\end{align*}
Hence, 
$$SD(\mathcal{D} \oplus \mathcal{M}, \mathcal{U}) 
\leq \frac{\alpha \sqrt{N}}{2^{1+k/2}}$$
\end{proof}

\begin{theorem}
Let $M$ be a distribution with min entropy $k$ over $\{0 , 1 \}^n$, let $G_0 \sim 1 \times \frac{n}{2}$, $G \sim \frac{n}{2} \times n$, and let $X$ be a uniform distribution over $\{ 0, 1 \} ^ \frac{n}{2}$. Then 
$$SD \{ (XG_0, XG \oplus M, G_0, G), (U, XG \oplus M, G_0, G) \} \leq 
\frac{1}{2^{1+k/2+n/4}}$$
\end{theorem}

\begin{proof}
For convenience, let $A_{G_0, G} = (XG_0, XG \oplus M | G_0 , G)$, 
$A'_{G_0,G} = (XG_0, XG | G_0 , G) $ and
$B_{G_0,G} = (U, XG \oplus M | G_0 , G)$.

\begin{claim}
For any $S \subseteq [n+1]$, $\widehat{A_{G_0,G}}(S) = \widehat{B_{G_0,G}}(S)$ if $S_1 = 0$, and 
$\widehat{B_{G_0,G}}(S) = 0$ if $S_1 = 1$.
\end{claim} 

\begin{claim}
For any set $S \subseteq [n+1]$ with $S_1 =1$, 
$$\underset{G_0, G} {\mathbb{E}} 
	[\widehat{A'_{G_0,G}}(S)^2] \leq \frac{1}{(2N)^2 \cdot \sqrt N} $$
\end{claim}

\noindent \textit{Proof.}
$$ \widehat{A'_{G_0,G}}(S) = \mathbb{E}[A'_{G_0,G}(x) \cdot \chi_S(x)]
= \frac{1}{2N} \text{bias}_S(A'_{G_0, G})$$

\begin{align*}
\text{where }
\text{bias}_S(A'_{G_0, G})
&= \frac{1}{2N} \left| 
	A'_{G0,G} \{ x : \underset{i \in S} {\oplus xG_{i-1}} = 0 \} - 
	A'_{G0,G} \{ x : \underset{i \in S} {\oplus xG_{i-1}} = 1 \} 
	\right| \\
&= \frac{1}{2N} \left| 
	A'_{G0,G} \{ x : \underset{i \in S} {x \cdot (\oplus G_{i-1}}) = 0 \} - 
	A'_{G0,G} \{ x : \underset{i \in S} {x \cdot (\oplus G_{i-1}}) = 1 \} 
	\right| \\
\end{align*}
If $G_0 = \underset{i \in S_{>1}} {\oplus G_{i-1}}$, then $\text{bias}_S (A'_{G_0, G}) = 1$, and if $G_0 \neq  \underset{i \in S_{>1}} {\oplus G_{i-1}}$ then $\text{bias}_S (A'_{G_0, G}) = 0$. \\
$$Pr[G_0 =  \underset{i \in S_{>1}} {\oplus G_{i-1}}]  
\leq Pr[G_0 \in {\langle G_{i-1} : i \in S \rangle}] \leq \frac{1}{2^{n/2}}= \frac{1}{\sqrt{N}}$$
$$Pr[G_0 \neq  \underset{i \in S_{>1}} {\oplus G_{i-1}}]  
\geq Pr[G_0 \in {\langle G_{i-1} : i \in S \rangle}] \geq 1 - \frac{1}{\sqrt{N}}$$

$$\underset{G_0, G} {\mathbb{E}} 
	[\widehat{A'_{G_0,G}}(S)^2] \leq \frac{1}{\sqrt{N}} \cdot (\frac{1}{2N})^2 
	= \frac{1}{(2N)^2 \cdot \sqrt{N}}$$

Back to the problem,
\begin{align*}
& SD \{ (XG_0, XG \oplus M, G_0, G), (U, XG \oplus M, G_0, G) \} \\
= & \underset{G_0, G} {\mathbb{E}} \left[
	SD (A_{G_0,G},B_{G_0,G}) \right] \\
= & \underset{G_0, G} {\mathbb{E}} \left[
	\frac{1}{2} \sum\limits_{i,j} | A_{G_0,G}(i)- B_{G_0,G} (i)| \right] \\
\leq & \frac{1}{2} \underset{G_0, G} {\mathbb{E}} \left[ \sqrt{
	2N \cdot (2N)^2 \cdot \| A_{G_0,G} - B_{G_0,G} \|_2^2 }  \right] \\	
\leq & \frac{(2N)^{3/2}}{2} \sqrt{ \underset{G_0, G} {\mathbb{E}} \left[ 
	\sum\limits_{i} (A_{G_0,G}(i) - B_{G_0,G}(i) )^2 \right] } \\
= & \frac{(2N)^{3/2}}{2} \sqrt{
	\sum\limits_{S} \underset{G_0, G} {\mathbb{E}} \left[  
		(\widehat{A_{G_0,G}}(S) -
	     \widehat{B_{G_0,G}} (S) )^2 \right] }\\	
= & \frac{(2N)^{3/2}}{2} \sqrt{
	\sum\limits_{S_1 = 1} \underset{G_0, G} {\mathbb{E}} \left[  
		\widehat{A_{G_0,G}}(S)^2 \right] } \\
= & \frac{(2N)^{3/2}}{2} \sqrt{
	\sum\limits_{S_1 = 1} \underset{G_0, G} {\mathbb{E}} \left[  
		(\widehat{(O, M)}(S) \oplus
		\widehat{(XG_0, XG |G_0, G)}(S))^2 \right] } \\		
= & \frac{(2N)^{3/2}}{2} \sqrt{
	\sum\limits_{S_1 = 1} \underset{G_0, G} {\mathbb{E}} \left[  
		\widehat{(O, M)}(S)^2 \cdot 
		\widehat{(XG_0, XG |G_0, G)}(S)^2 \right] } \\
=& \frac{(2N)^{3/2}}{2} \sqrt{
	\sum\limits_{S_1 = 1} \widehat{(O, M)}(S)^2 \cdot 
		\underset{G_0, G} {\mathbb{E}} \left[  
		\widehat{A'_{G_0,G}}(S)^2 \right] } \\	
=& \frac{(2N)^{3/2}}{2} \sqrt{
	\sum\limits_{S_1 = 1} \widehat{(O, M)}(S)^2 \cdot 
		\frac{1} {(2N)^2 \cdot \sqrt{N}} }	\\
\leq &	\frac{(2N)^{3/2}}{2} \sqrt{
	\frac{1}{2N \cdot 2^k} \cdot 
		\frac{1} {(2N)^2 \cdot \sqrt{N}} }\\
= & \frac{1}{2^{1+k/2} \cdot N^{1/4}}						     		                                 
\end{align*}
\end{proof}

\chapter{Bourgain's Extractor}

\appendix

\chapter{}
\section{Dual of a Vector Space}
\begin{definition} [Dual space]
Let $V$ be a subspace of $\{0,1\}^n$. 
We define the dual of V as 
$V^\perp = \{ x \in \{ 0,1 \}^n | x \cdot v = 0 \ \forall v \in V \}$.
\end{definition}

\begin{theorem}
$V^\perp$ is a subspace of $\{ 0,1 \}^n$.
\end{theorem}

\begin{proof}
For any $x, y \in V^\perp, a \in \{0,1\}, (a \cdot x+y)\cdot v = a \cdot (x \cdot v) + y \cdot v = 0 + 0 = 0$. 
\end{proof} 

\begin{lemma} $\sum \limits_{i: \text{even}}^t {n \choose i} = 
\sum \limits_{i: \text{odd}}^t {n \choose i} = 2^{t-1} $.
\end{lemma}

\begin{theorem} For any subspace $V$ of dimension $k$ of $\{ 0,1 \}^n$, 
there exists a unique dual space $V^\perp$ of dimension $(n-k)$.
\end{theorem}

\begin{proof}
We will show that $|V^\perp| = 2^{n-k}$ by induction on $k$.\\
If $k=0$, then $V = \{ \textbf{0}\}$. Clearly, $V^\perp = \{ 0, 1 \}^n$. \\
If $k=1$, let $V = \{ \vec{0}, v \}$. 
Suppose the number of $v_i = 1$ is $t$, 
then the number of $x$ such that $x \cdot v = 0$ is 
$\sum \limits_{i: 2|t-i} {n \choose i} 2^{n-t} =
2^{t-1} \cdot 2^{n-t} = 2^{n-1}$ by Lemma A.3. \\
Suppose that there exists a unique orthogonal subspace $V^\perp$ 
of dimension $(n-k+1)$ for any subspace $V$ of dimension $k-1$ of $\{ 0,1 \}^n$, where $k \geq 2$. \\
Let $V=\langle v_1, v_2,..., v_{k} \rangle$, 
$S_1 = \langle v_1, v_2,..., v_{k-1}\rangle$, 
and $S_2 = \langle v_{k}\rangle$.  Then, $V^\perp = S_1^\perp \cap S_2^\perp$. \\
Suppose $dim(V^\perp) = t$. We want to show $t = n-k$.\\
By induction hypothesis, $dim(S_1^\perp) = n-k+1$ 
and $dim(S_2^\perp) = n-1$. \\
If $t \leq n-k-1$, then we need $[(n-k+1) - t]$ independent vectors to cover $S_1^\perp$ from extending $V^\perp$, and we need $[(n-1) -t]$ independent vectors to cover $S_2^\perp$ from extending $V^\perp$. Since $S_1^\perp \cup S_2^\perp \subseteq \{ 0,1\}^n$, we must have $[(n-k+1) - t] + [(n-1) -t] + t \leq n$, which is equivalent to $t \geq n-k$, contradiction.\\
If $t \geq n-k+1$, then $S_1^\perp \subseteq S_2^\perp$,
this is impossible since $v_k$ is independent from $v_1, v_2, ..., v_{k-1}$. \\
Thus, $t = n-k$. So $|V^\perp| = 2^{n-k}$. 
\end{proof}

\section{Statistical Distance between Two Joint Distributions}
\begin{theorem}
Let $A,B$ be some probability distributions over the same sample space, and let $C$ be a probability distribution. Then 
$$ SD \{(A,C), (B,C)\} 
= \underset{c \sim C} {\mathbb{E}} \left[ SD \{(A | C = c), (B | C=c)\} \right] $$
\end{theorem}
\begin{proof}
\begin{align*}
  SD \{(A,C), (B,C)\} 
& = \frac{1}{2} \sum\limits_{i,c} | (A,C)(i,c) - (B,C)(i,c)| \\
& = \frac{1}{2} \sum\limits_{i,c} | Pr(C = c) \cdot Pr(A = i | C =c) -
	                                Pr(C = c) \cdot Pr(B = i | C =c) |\\ 
& = \sum\limits_{c \sim C} \left( Pr(C = c) \cdot  \frac{1}{2} \sum\limits_{i} |Pr(A = i | C =c) - Pr(B = i | C =c) | \right) \\
& = \sum\limits_{c \sim C}  Pr(C = c) \cdot 
	SD \{(A | C = c), (B | C= c) \} \\ 
& = \underset{c \sim C} {\mathbb{E}} \left[ SD \{(A | C = c), (B | C=c)\} \right]		                                
\end{align*}
\end{proof}

\begin{claim}% \widehat{C}(S_C) \cdot \widehat{D | C}(S_D)
For any distributions $C,D$, 
$$\widehat{(C,D)}(S) = \cdots  \text{, where } S = S_CS_D $$
\end{claim}

\begin{proof}
\begin{align*}
\widehat{(C,D)}(S) 
& = \langle (C,D), \chi_S \rangle \\
& = \frac{1}{2N} \sum\limits_{(c,d)}
	(C,D)(c,d) \cdot \chi_S(c,d) \\
& = \frac{1}{2N} \sum\limits_{(c,d)}
	C(c) \cdot (D | C = c)(d) \cdot \chi_{S_C}(c) \cdot \chi_{S_D}(d) \\
& = \frac{1}{2N} \sum\limits_{c} \left
				 [C(c) \cdot \chi_{S_C}(c) \cdot 
				 \sum\limits_{d} (D | C = c)(d)	\chi_{S_D}(d) \right] \\
& = \frac{1}{2} \sum\limits_{c} \left
				 [C(c) \cdot \chi_{S_C}(c) \cdot 
				 \widehat{(D|C=c) }(S_D) \right] \\	
\end{align*}							
\end{proof}

\section{Group Basics}
\subsection{Notation}
We reverse the variable $p$ to denote primes.

$\mathbb{F}_p$ denotes the field of size $p$.

$G$ denotes a finite abelian group. 

$\mathbb{C}$ denotes the set of complex numbers.

\begin{definition}
We say $\psi : G \rightarrow \mathbb{C}$ is a character if $\psi$ is a homomorphism. 
\end{definition}

\begin{definition}
We say a map $e : G \times G  \rightarrow \mathbb{C}$ is a bilinear map if it is a homomorphism in each variable. 
\end{definition}
\begin{theorem}
For every abelian group $G$, there exists a symmetric non-degenerate bilinear $e : G \times G \rightarrow \mathcal{C}$ 
\end{theorem}
\subsection{Dual of a finite Abelian Group}
\begin{theorem}
Every ï¬nite abelian group $G$ is isomorphic to its character group $G^\land$
\end{theorem}

\section{Product Graph}
\begin{definition}
$D = (a,b,c,d)$ is a $2 \times 2$ distribution graph if and only if 
\begin{itemize}
\item [1.] $a+b+c+d=1$,
\item [2.] $a,b,c,d \in [0,1]$.   
\end{itemize}
\end{definition}

\begin{definition}
$G = (x,y,z,t)$ is a $2 \times 2$ product graph if and only if 
\begin{itemize}
\item [1.] $G$ is a distribution graph,
\item [2.] $xt = yz$, or $x = t = 0$, or $y =z = 0$.
\end{itemize}
\end{definition}

 
Let $\mathbb{G}$ be the space of all $2 \times 2$ product graphs and let $\mathbb{D}$ be the space of all $2 \times 2$ distribution graphs. We want to find 
$$ D^* = \underset{D \in \mathbb{D}} {argmax} \ dist(G, \mathbb{G})$$
$$m = \underset{D \in \mathbb{D}} {\max} \ dist(G, \mathbb{G})$$
Let $D = (a,b,c,d)$ be any $2 \times 2$ distribution graph. Without lost of generality, assume $a \geq d$,  $b \geq c$, and $ad \geq bc$ 

\begin{claim}
$dist(D, \mathbb{G}) \leq f(a,b,c,d)$, \\
where $f(a,b,c,d) = \min \{(b+c), \frac{(ad - bc)}{a+b},
\frac{1}{2}(| \sqrt{a} - a - b| + |\sqrt{a} - a - c|, |(1 - \sqrt{a})^2 -d|) \}$
\end{claim}

\begin{proof}
Let $G = (x,y,z,t)$ be a product graph.  
$$dist(D,\mathbb{G}) \leq dist(D,G) = \frac{1}{2} (|a-x| + |b-y| + |c-z| + |d-t|)$$
So if we can find some graphs $G's$ such that $dist(D,G)$ equal to the three values above respectively, we are done. From the second property of distribution graph, it suggests the way to choose such $G's$.
\begin{itemize}
\item [1.] Choose $y = z =0$, $x = a+b$, and $t = c+d$, then $dist(D,G) = b+c$.
\item [2.] Choose $xt = yz$, $x = a$, $y =b$, $z = \frac{(c+d)a}{a+b}$, and $\frac{(c+d)b}{a+b}$, then 
$$dist(D,G) = \frac{1}{2}(|c - \frac{(c+d)a}{a+b}| + |d - \frac{(c+d)b}{a+b}|
= \frac{(ad - bc)}{a+b} $$
\item [3.] Choose $x = a$, $y = z = \sqrt{a} - a$, and $t = (1 - \sqrt{a})^2$, then 
$$dist(D,G) = \frac{1}{2}(| \sqrt{a} - a - b| + |\sqrt{a} - a - c|, |(1 - \sqrt{a})^2 -d|) $$
\end{itemize}
\end{proof}

\begin{claim}
$m \leq \underset {a,b,c,d} {\max} f(a,b,c,d)$
\end{claim} 
\begin{claim} 
$\underset {a,b,c,d} {\max} f(a,b,c,d) = \sqrt{5} -2$
\end{claim}

\begin{proof}
\begin{itemize}
\item [1.] If $b(1-d) \geq ad$, then 
$\frac{(ad - bc)}{a+b} = \frac{(ad - bc)}{1-c-d} \leq \frac{ad}{1 - d} 
\leq \frac{(\frac{a+d}{2})^2}{1 - (\frac{a+d}{2})}$. Then 
$$\underset {a,b,c,d} {\max} f(a,b,c,d) \leq 
\max \min \{(1-2h), \frac{h^2}{1-h} \}$$
where $h = (a+d)/2$
\item [2.] If $b(1-d) = ba + b^2 + bc \leq ad$, suppose $\max \min \{(1-2h), \frac{h^2}{1-h} \} > \sqrt{5} -2$. \\
Then $\sqrt{a} \geq a + b \geq a + c$ since $a \geq (a+b)^2 \Leftrightarrow a(a+b+c+d) \geq a^2 + 2ab + b^2 \Leftrightarrow ac + ad \geq ab + b^2$, which is true. So $d \geq (1 -\sqrt{a})^2$. Thus, 
\begin{align*}
& \frac{1}{2}(| \sqrt{a} - a - b| + |\sqrt{a} - a - c|, |(1 - \sqrt{a})^2 -d|) \\
&= \frac{1}{2} (2 \sqrt{a} - 2a - (1 - a -d) + d - (1 - \sqrt{a})^2 \\
&= d - (1 - \sqrt{a})^2 \\
&= (\sqrt{d} + \sqrt{a} - 1)(1 + \sqrt{d} - \sqrt{a}) \\
&\leq \sqrt{d} + \sqrt{a} - 1\\
& \leq \sqrt{2(a+d)} - 1
\end{align*}
Hence, $$\underset {a,b,c,d} {\max} f(a,b,c,d) \leq 
\max \min \{1-2h, 2 \sqrt{h} -1 \} \}$$
\end{itemize}
\begin{figure}
	\centering
    \includegraphics[scale = 0.8]{graph.jpg}
\end{figure}
From the graph, we can see that 
$$\max \min \{1-2h, 2 \sqrt{h} -1, \frac{h^2}{1-h} \} = \sqrt{5} -2$$
when $a = d = \frac{3 - \sqrt{5}}{2}$, $b = \sqrt{5} - 2$, and $c = 0$
\end{proof}

\end{document}

